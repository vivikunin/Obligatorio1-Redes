{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Obligatorio 1\n",
        "\n"
      ],
      "metadata": {
        "id": "VDEocYyNiTk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import numpy.random as rnd\n",
        "\n",
        "# seteamos la semilla de generación de números aleatorios para reproducir los resultados\n",
        "rnd.seed(2025)\n",
        "\n",
        "m= 20\n",
        "n= 10\n",
        "A = rnd.rand(m,n)\n",
        "b = rnd.rand(m)\n",
        "lambd = 0.8"
      ],
      "metadata": {
        "id": "T-DSShm1jUOd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sea $\\lambda$ un real positivo. Considere la función $f:\\mathbb{R}^n \\to \\mathbb{R}$ definida como $f(x) = ||Ax-b||^2 + \\lambda||x||^2$, donde $A \\in \\mathbb{R}^{m\\times n}$ es una matriz, y $b \\in \\mathbb{R}^m$ un vector dado.\n",
        "\n",
        "\n",
        "\n",
        "1.   Calcule el gradiente de la función ($\\nabla{f}$). Escriba la expresión en esta celda de texto, y en la función de la celda de código siguiente.\n",
        "\n",
        "$\\nabla f(x) =\n",
        "$$\n",
        "\\ 2A^T A x - 2A^T b + 2\\lambda x\n",
        "$\n",
        "\n",
        "Explicacion: Usando lo visto en el teórico planteamos\n",
        "$$\n",
        "f(a + h) = \\|A(X + h) - b\\|^2 + \\lambda \\|X + h\\|^2\n",
        "$$\n",
        "\n",
        "Usando que la norma al cuadrado es igual al producto interno de la matriz consigo misma obtuvimos que lo anterior es igual a:\n",
        "\n",
        "$$\n",
        "= \\langle Ax + Ah - b,\\ Ax + Ah - b \\rangle + \\lambda \\langle x + h,\\ x + h \\rangle\n",
        "$$\n",
        "\n",
        "Desarrollando con las propiedades de producto interno llegamos a la expresion:\n",
        "\n",
        "$$\n",
        "= \\langle Ax - b, Ax - b \\rangle + 2 \\langle Ax - b, Ah \\rangle + 2\\langle Ah, Ah \\rangle + \\lambda \\left( 2\\langle x, x \\rangle + 2 \\langle h,h \\rangle +2 \\langle x, h \\rangle \\right)\n",
        "$$\n",
        "\n",
        "Los términos:\n",
        "$$ \\langle Ax - b, Ax - b \\rangle $$ y $$\\ 2\\lambda \\langle x, x \\rangle $$ son iguales a f(x) ya que f(x) es igual a la norma cuadrada de\n",
        "$$\n",
        "f(a) = \\|A(X) - b\\|^2 + \\lambda \\|X\\|^2\n",
        "$$.\n",
        "\n",
        "A su vez los términos:\n",
        "$$ 2\\langle h,h \\rangle $$\n",
        "y\n",
        "$$ 2\\langle Ah, Ah \\rangle $$\n",
        "son\n",
        "$$ O(h^2 ) $$\n",
        "y corresponden al resto del desarrollo de Taylor.\n",
        "\n",
        "Los términos restantes de la expresión son lineales:\n",
        "$$\n",
        "= 2 \\langle Ax - b, Ah \\rangle +2 \\lambda  \\langle x, h \\rangle\n",
        "$$\n",
        "Por definición deferenciabilidad, una función f(x) es diferenciable si puede escribirse como f(x) más un término lineal en h más un término cuadrático en h, donde el término lineal en h es el producto interno del gradiente con h.\n",
        "\n",
        "Despejando ahora la h del lado derecho del producto interno obtenemos:\n",
        "$$\n",
        "= 2 \\langle A^T(Ax - b), h \\rangle +2 \\lambda  \\langle x, h \\rangle\n",
        "$$\n",
        "donde\n",
        "$$ A^T(Ax - b)+ 2 \\lambda  x $$\n",
        "es el gradiente.\n",
        "\n",
        "De esta forma obtuvimos la solución de arriba:\n",
        "\n",
        "$\\nabla f(x) =\n",
        "$$\n",
        "\\ 2A^T A x - 2A^T b + 2\\lambda x\n",
        "$\n",
        "\n"
      ],
      "metadata": {
        "id": "m6fRr9czemRp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LZhI3S7Mmly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KFpqzWHn4AvC"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  return la.norm(A@x-b)**2 + lambd*la.norm(x)**2\n",
        "\n",
        "def gradf(x):\n",
        "  ## completar\n",
        "  return 2*A.T@A@x-2*A.T@b+2*lambd*x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.   Compruebe numéricamente que el cálculo del gradiente es correcto, escribiendo una función que calcule una aproximación numérica del gradiente, a partir de cocientes incrementales.\n",
        "\n"
      ],
      "metadata": {
        "id": "mgQw_pHoi02A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_numerico(x):\n",
        "  eps = 1e-7\n",
        "  grad = np.zeros(n)\n",
        "  for i in range(len(x)):\n",
        "    xcopia=x.copy()\n",
        "    xcopia[i]+=eps\n",
        "    grad[i]=(f(xcopia)-f(x))/eps\n",
        "  return grad\n",
        "\n",
        "xr = rnd.rand(n)\n",
        "\n",
        "#calculemos la norma de la diferencia entre el gradiente calculado y la aproximación numérica, en un punto aleatorio xr.\n",
        "#Si todo está bien, debería ser un número chico\n",
        "print(la.norm(gradf(xr) - grad_numerico(xr)))"
      ],
      "metadata": {
        "id": "bPBFKCe8jFG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab2afbe-f6f6-402a-adee-1862bee69e47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8455308629281004e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  A partir de la expresión de $\\nabla f$ hallada en el punto 1, escriba la condición de optimalidad, y halle un $x^*$ que la cumpla.\n",
        "\n",
        "La condición de optimalidad es:\n",
        "Si f es diferenciable en x* y x* es un mínimo relativo, entonces ∇f(x*)=0.\n",
        "\n",
        "Comenzamos igualando el gradiente obtenido arriba a 0:\n",
        "$$\\nabla f(x^*) =\n",
        "\\ 2A^T A x^* - 2A^T b + 2\\lambda x* =0\n",
        "$$\n",
        "Luego simplificamos los coeficientes 2 y despejamos los términos en x:\n",
        "\n",
        "$$\n",
        "\\ A^T A x* + \\lambda x^* = A^T b\n",
        "$$\n",
        "\n",
        "El siguiente paso fue factorizar x* obteniendo:\n",
        "$$\n",
        "\\ (A^T A + I\\lambda) x^* = A^T b\n",
        "$$\n",
        "Despejamos x^* multiplicando a ambos lados por el inverso de la matriz y obtuvimos el resultado final:\n",
        "$$\n",
        " x^* = (A^T A + I\\lambda)^{-1}A^T b\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "PM8UJdioqEM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sustituya 0 por el vector hallado que cumpla la condición de optimalidad\n",
        "xstar = la.inv((A.T@A)+(np.eye(n)*lambd))@(A.T@b)"
      ],
      "metadata": {
        "id": "6F8_jClVqhBI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Compruebe que el gradiente se anula en $x^*$"
      ],
      "metadata": {
        "id": "3JxpUp9tquyn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpwfHj9UYB8-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En esta celda, comprobar la condición necesaria de optimalidad en xstar\n",
        "print(la.norm(gradf(xstar)))"
      ],
      "metadata": {
        "id": "B_M5c-aGtQDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237b0951-ff9e-4fdd-ee68-b415d3ef51f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.675286015406637e-14\n"
          ]
        }
      ]
    }
  ]
}